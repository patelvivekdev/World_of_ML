{
  
    
        "post0": {
            "title": "Logistic Regression",
            "content": "0. Data Preprocessing . Importing the libraries . import numpy as np import matplotlib.pyplot as plt import pandas as pd . Importing the dataset . dataset = pd.read_csv(&#39;Social_Network_Ads.csv&#39;) dataset . User ID Gender Age EstimatedSalary Purchased . 0 15624510 | Male | 19 | 19000 | 0 | . 1 15810944 | Male | 35 | 20000 | 0 | . 2 15668575 | Female | 26 | 43000 | 0 | . 3 15603246 | Female | 27 | 57000 | 0 | . 4 15804002 | Male | 19 | 76000 | 0 | . ... ... | ... | ... | ... | ... | . 395 15691863 | Female | 46 | 41000 | 1 | . 396 15706071 | Male | 51 | 23000 | 1 | . 397 15654296 | Female | 50 | 20000 | 1 | . 398 15755018 | Male | 36 | 33000 | 0 | . 399 15594041 | Female | 49 | 36000 | 1 | . 400 rows √ó 5 columns . Check if any null value . dataset.isna().sum() . User ID 0 Gender 0 Age 0 EstimatedSalary 0 Purchased 0 dtype: int64 . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 User ID 400 non-null int64 1 Gender 400 non-null object 2 Age 400 non-null int64 3 EstimatedSalary 400 non-null int64 4 Purchased 400 non-null int64 dtypes: int64(4), object(1) memory usage: 15.8+ KB . dataset.drop(&#39;User ID&#39;, axis=1, inplace=True) . dataset.head() . Gender Age EstimatedSalary Purchased . 0 Male | 19 | 19000 | 0 | . 1 Male | 35 | 20000 | 0 | . 2 Female | 26 | 43000 | 0 | . 3 Female | 27 | 57000 | 0 | . 4 Male | 19 | 76000 | 0 | . Split into X &amp; y . X = dataset.drop(&#39;Purchased&#39;, axis=1) X.head() . Gender Age EstimatedSalary . 0 Male | 19 | 19000 | . 1 Male | 35 | 20000 | . 2 Female | 26 | 43000 | . 3 Female | 27 | 57000 | . 4 Male | 19 | 76000 | . y = dataset[&#39;Purchased&#39;] y.head() . 0 0 1 0 2 0 3 0 4 0 Name: Purchased, dtype: int64 . Convert categories into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_feature = [&quot;Gender&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_feature)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) . pd.DataFrame(transformed_X).head() . 0 1 2 3 . 0 0.0 | 1.0 | 19.0 | 19000.0 | . 1 0.0 | 1.0 | 35.0 | 20000.0 | . 2 1.0 | 0.0 | 26.0 | 43000.0 | . 3 1.0 | 0.0 | 27.0 | 57000.0 | . 4 0.0 | 1.0 | 19.0 | 76000.0 | . Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size = 0.25, random_state = 2509) . Feature Scaling . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . 1.Training the model on the Training set . from sklearn.linear_model import LogisticRegression classifier = LogisticRegression() classifier.fit(X_train, y_train) . LogisticRegression() . LR_score = classifier.score(X_test, y_test) LR_score . 0.89 . 2.Predicting the Test set results . y_pred = classifier.predict(X_test) . Making the Confusion Matrix . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) print(cm) . [[65 5] [ 6 24]] .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/classification/2020/10/21/logistic_regression.html",
            "relUrl": "/jupyter/classification/2020/10/21/logistic_regression.html",
            "date": " ‚Ä¢ Oct 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Preprocessing",
            "content": "1.Import library . import numpy as np import pandas as pd import matplotlib.pyplot as plt . 2.Get the data . car_sales = pd.read_csv(&quot;car-sales-data.csv&quot;) car_sales.head() . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . 3.Check for missing values . car_sales.isna().sum() . Make 49 Colour 50 Odometer (KM) 50 Doors 50 Price 50 dtype: int64 . 3.1 What if data is filled with missing values? . Drop the rows with no labels | Fill them with some value(also known as imputation). | 1.Drop the rows with no labels . car_sales.dropna(subset=[&quot;Price&quot;],inplace=True) car_sales.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 NaN | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 950 rows √ó 5 columns . 2.Fill them with some value . Option 1. Fill missing data with pandas | Option 2. Fill missing data with scikit learn | . 2.1 Option 1. With Pandas . car_sales.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 950 entries, 0 to 999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 Make 903 non-null object 1 Colour 904 non-null object 2 Odometer (KM) 902 non-null float64 3 Doors 903 non-null float64 4 Price 950 non-null float64 dtypes: float64(3), object(2) memory usage: 44.5+ KB . car_sales[&quot;Make&quot;].fillna(&quot;missing&quot;,inplace=True) car_sales[&quot;Colour&quot;].fillna(&quot;missing&quot;,inplace=True) car_sales[&quot;Odometer (KM)&quot;].fillna(car_sales[&quot;Odometer (KM)&quot;].median(),inplace=True) car_sales[&quot;Doors&quot;].fillna(4,inplace=True) . car_sales . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 missing | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 950 rows √ó 5 columns . car_sales.isna().sum() . Make 0 Colour 0 Odometer (KM) 0 Doors 0 Price 0 dtype: int64 . 2.2 Option 2. With scikit learn . car_sales_missing = pd.read_csv(&quot;car-sales-data.csv&quot;) car_sales_missing . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . ... ... | ... | ... | ... | ... | . 995 Toyota | Black | 35820.0 | 4.0 | 32042.0 | . 996 NaN | White | 155144.0 | 3.0 | 5716.0 | . 997 Nissan | Blue | 66604.0 | 4.0 | 31570.0 | . 998 Honda | White | 215883.0 | 4.0 | 4001.0 | . 999 Toyota | Blue | 248360.0 | 4.0 | 12732.0 | . 1000 rows √ó 5 columns . car_sales_missing.dropna(subset=[&quot;Price&quot;],inplace=True) car_sales_missing.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . car_sales_missing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 950 entries, 0 to 999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 Make 903 non-null object 1 Colour 904 non-null object 2 Odometer (KM) 902 non-null float64 3 Doors 903 non-null float64 4 Price 950 non-null float64 dtypes: float64(3), object(2) memory usage: 44.5+ KB . If you are use scikit learn to fill missing value, then you have to Split data into X and y . X = car_sales_missing.drop(&quot;Price&quot;,axis=1) y = car_sales_missing[&quot;Price&quot;] . from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer # Fill categorical values with &#39;missing&#39; &amp; numerical values with median cat_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=&quot;missing&quot;) door_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=4) num_imputer = SimpleImputer(strategy=&quot;median&quot;) # Define columns cat_features = [&quot;Make&quot;, &quot;Colour&quot;] door_feature = [&quot;Doors&quot;] num_features = [&quot;Odometer (KM)&quot;] # Create an imputer (something that fills missing data) imputer = ColumnTransformer([ (&quot;cat_imputer&quot;, cat_imputer, cat_features), (&quot;door_imputer&quot;, door_imputer, door_feature), (&quot;num_imputer&quot;, num_imputer, num_features) ]) filled_X = imputer.fit_transform(X) filled_X . array([[&#39;Honda&#39;, &#39;White&#39;, 4.0, 35431.0], [&#39;BMW&#39;, &#39;Blue&#39;, 5.0, 192714.0], [&#39;Honda&#39;, &#39;White&#39;, 4.0, 84714.0], ..., [&#39;Nissan&#39;, &#39;Blue&#39;, 4.0, 66604.0], [&#39;Honda&#39;, &#39;White&#39;, 4.0, 215883.0], [&#39;Toyota&#39;, &#39;Blue&#39;, 4.0, 248360.0]], dtype=object) . car_sales_filled = pd.DataFrame(filled_X, columns=[&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;, &quot;Odometer (KM)&quot;]) car_sales_filled.isna().sum() . Make 0 Colour 0 Doors 0 Odometer (KM) 0 dtype: int64 . car_sales_filled . Make Colour Doors Odometer (KM) . 0 Honda | White | 4 | 35431 | . 1 BMW | Blue | 5 | 192714 | . 2 Honda | White | 4 | 84714 | . 3 Toyota | White | 4 | 154365 | . 4 Nissan | Blue | 3 | 181577 | . ... ... | ... | ... | ... | . 945 Toyota | Black | 4 | 35820 | . 946 missing | White | 3 | 155144 | . 947 Nissan | Blue | 4 | 66604 | . 948 Honda | White | 4 | 215883 | . 949 Toyota | Blue | 4 | 248360 | . 950 rows √ó 4 columns . Convert categorical data into numbers . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) # Fill train and test values separately transformed_X = transformer.fit_transform(car_sales_filled) # Check transformed and filled X_train transformed_X.toarray() . array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 3.54310e+04], [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00, 1.00000e+00, 1.92714e+05], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 8.47140e+04], ..., [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00, 0.00000e+00, 6.66040e+04], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.15883e+05], [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.48360e+05]]) . Split data into train and test . from sklearn.model_selection import train_test_split np.random.seed(2509) X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2) . X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((760, 15), (190, 15), (760,), (190,)) . Now data is in write shape to fit into model .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/data%20preprocessing/2020/10/21/Data_preprocessing.html",
            "relUrl": "/jupyter/data%20preprocessing/2020/10/21/Data_preprocessing.html",
            "date": " ‚Ä¢ Oct 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Basic machine learning path",
            "content": "Basic path to learn Practical Machine Learning. . Prerequisite . High school math(vectors, matrices, calculus, probability, and stats) | Basic Python Help. | Must have Patience to learn new things. | . . Motivation . Watch AI For Everyone By Andrew Ng . The meaning behind common AI terminology, including neural networks, machine learning, deep learning, and data science. | What AI realistically can‚Äìand cannot‚Äìdo. | How to spot opportunities to apply AI to problems in your own organization. | What it feels like to build machine learning and data science projects. | How to work with an AI team and build an AI strategy in your company. | How to navigate ethical and societal discussions surrounding AI. | . | YouTube Originals AGE OF AI . How AI is used in real life. . | . Started learning . Step-0 . Understand basic of machine learning Supervised Learning | Unsupervised Learning | Classification and Regression | . | Learn python and some useful library Pandas Pandas is a popular Python library for data analysis. . | NumPy NumPy is a very popular python library for large multi-dimensional array and matrix processing. . | Matplotlib Matpoltlib is a very popular Python library for data visualization. . | . | Setup Local Machine with latest Anaconda Anaconda is a free and open-source distribution of the Python and R. . | Step-1 . How to use data from verious source like [Kaggle UCI ml repo]. . | Start to use Jupyter notebook A complate IDE for data science and machine learning. . | Started hand on practice with scikit-learn. . | Use scikit-learn map and documentation. . | Step-2 . Now, you are a little bit comfortable with coding it‚Äôs time to learn basic maths behind those algorithms. . | Take a Andrew‚Äôs Course. . | . Step-3 . Learn about Deep-learning. . | Learn about popular library [TensorFlow PyTorch] TensorFlow is backed by Google Brain team. | PyTorch is developed by Facebook‚Äôs AI Research lab. | Both have large community. | There are other librarys as well like Theano, Keras, Caffe, Apache MXNet and many more. | . | In neural network learn ANN (artificial neural network) | CNN (Convolutional neural network) | RNN (Recurrent neural networks) | Autoencoder | | . . Happy coding and have a great time learning how to make machines smarter. .",
            "url": "https://vivek2509.github.io/World_of_ML/markdown/2020/10/19/Basic-ML-path.html",
            "relUrl": "/markdown/2020/10/19/Basic-ML-path.html",
            "date": " ‚Ä¢ Oct 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Start writing blog",
            "content": "Start writing blog within 10 min . How to start write blog direct from Github using Markdown and Jupyter notebook. . . Requirement . github account. | . Follow step . Use fastai/fastpages template. Within 20-30 sec one pull request is generated by github-action. | Follow step and create SSH key from this and add public and private key. | Merge pull request and wait for 2-3 min for github-action to build the site. | Change Name and description . open _config.yml in edit mode. | change name and description | you can add social links as well | . . . . Adding stuff . Make sure are you add stuff into direct master OR main branch. | You can add notebook, markdown and words file into _notebook , _posts and _word folder accordingly with right name formet. | You can upload all your local images in images folder | . . . . Happy blogging .",
            "url": "https://vivek2509.github.io/World_of_ML/markdown/2020/10/18/How-to-start-blog-from-fastpage.html",
            "relUrl": "/markdown/2020/10/18/How-to-start-blog-from-fastpage.html",
            "date": " ‚Ä¢ Oct 18, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Email Spam Detector",
            "content": "Import library . import pandas as pd import numpy as np import matplotlib.pyplot as plt . Import dataset . email = pd.read_csv(&#39;emails.csv&#39;) . email.head(10) . text spam . 0 Subject: naturally irresistible your corporate... | 1 | . 1 Subject: the stock trading gunslinger fanny i... | 1 | . 2 Subject: unbelievable new homes made easy im ... | 1 | . 3 Subject: 4 color printing special request add... | 1 | . 4 Subject: do not have money , get software cds ... | 1 | . 5 Subject: great nnews hello , welcome to medzo... | 1 | . 6 Subject: here &#39; s a hot play in motion homela... | 1 | . 7 Subject: save your money buy getting this thin... | 1 | . 8 Subject: undeliverable : home based business f... | 1 | . 9 Subject: save your money buy getting this thin... | 1 | . len(email) . 5728 . email.isna().sum() . text 0 spam 0 dtype: int64 . email.shape . (5728, 2) . email.tail() . text spam . 5723 Subject: re : research and development charges... | 0 | . 5724 Subject: re : receipts from visit jim , than... | 0 | . 5725 Subject: re : enron case study update wow ! a... | 0 | . 5726 Subject: re : interest david , please , call... | 0 | . 5727 Subject: news : aurora 5 . 2 update aurora ve... | 0 | . Cleaning the text . import re import nltk nltk.download(&#39;stopwords&#39;) from nltk.corpus import stopwords from nltk.stem.porter import PorterStemmer corpus = [] for i in range(0, len(email)): e_mail = re.sub(&#39;[^a-zA-Z]&#39;, &#39; &#39;, email[&#39;text&#39;][i]) e_mail = e_mail.split() ps = PorterStemmer() e_mail = [ps.stem(word) for word in e_mail if not word in set(stopwords.words(&#39;english&#39;))] e_mail = &#39; &#39;.join(e_mail) corpus.append(e_mail) . [nltk_data] Downloading package stopwords to [nltk_data] C: Users patel AppData Roaming nltk_data... [nltk_data] Package stopwords is already up-to-date! . corpus[2509] . &#39;subject enron mid year perform manag process enron mid year perform manag process begun process requir select suggest review provid perform relat feedback may also request provid feedback fellow employe need access perform manag system pep http pep enron com question direct pep help desk follow number u option europ option canada canada employe e mail question perfmgmt enron com log pep enter user id password provid log immedi prompt chang secur password user id password user id wkamin password welcom&#39; . Creating the Bag of Words model . from sklearn.feature_extraction.text import CountVectorizer cv = CountVectorizer() X = cv.fit_transform(corpus).toarray() y = email[&#39;spam&#39;] . len(X) . 5728 . X.shape . (5728, 25607) . len(y) . 5728 . Splitting the dataset into the Training set and Test set . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0) . Training the Naive Bayes model on the Training set . 1.GaussianNB . from sklearn.naive_bayes import GaussianNB GN_classifier = GaussianNB() GN_classifier.fit(X_train, y_train) . GaussianNB() . GN_score = GN_classifier.score(X_test,y_test) GN_score . 0.9607329842931938 . y_GN_pred = GN_classifier.predict(X_test) . y_GN_pred . array([0, 0, 1, ..., 0, 0, 1], dtype=int64) . 2.MultinomialNB . from sklearn.naive_bayes import MultinomialNB MN_classifier = MultinomialNB() MN_classifier.fit(X_train, y_train) . MultinomialNB() . MN_score = MN_classifier.score(X_test,y_test) MN_score . 0.9825479930191972 . y_MN_pred = MN_classifier.predict(X_test) . Making the Confusion Matrix . from sklearn.metrics import confusion_matrix GN_cm = confusion_matrix(y_test, y_GN_pred) print(GN_cm) . [[875 10] [ 35 226]] . from sklearn.metrics import confusion_matrix MN_cm = confusion_matrix(y_test, y_MN_pred) print(MN_cm) . [[870 15] [ 5 256]] . Compare Both models . models = pd.DataFrame({&quot;GaussianNB&quot;: GN_score, &quot;MultinomialNB&quot;: MN_score }, index=[0]) models.T.plot.bar(title=&quot;Comapre different models&quot;, legend=False) plt.xticks(rotation=0); . SAVE MODEL . import pickle pickle.dump(MN_classifier,open(&quot;Email_spam_naive_bayes_MN.pkl&quot;,&quot;wb&quot;)) .",
            "url": "https://vivek2509.github.io/World_of_ML/jupyter/2020/10/10/Email_spam.html",
            "relUrl": "/jupyter/2020/10/10/Email_spam.html",
            "date": " ‚Ä¢ Oct 10, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". AI/ML enthusiast with ‚ô• in photography üì∏. .",
          "url": "https://vivek2509.github.io/World_of_ML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://vivek2509.github.io/World_of_ML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}